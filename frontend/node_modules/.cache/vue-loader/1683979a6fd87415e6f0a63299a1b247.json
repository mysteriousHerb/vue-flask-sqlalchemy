{"remainingRequest":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\node_modules\\vue-loader\\lib\\index.js??vue-loader-options!C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\src\\components\\FaceDetection.vue?vue&type=script&lang=js&","dependencies":[{"path":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\src\\components\\FaceDetection.vue","mtime":1560634986332},{"path":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\node_modules\\cache-loader\\dist\\cjs.js","mtime":1560457563834},{"path":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\node_modules\\babel-loader\\lib\\index.js","mtime":1560457563473},{"path":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\node_modules\\cache-loader\\dist\\cjs.js","mtime":1560457563834},{"path":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\node_modules\\vue-loader\\lib\\index.js","mtime":1560457580480}],"contextDependencies":[],"result":["//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n\r\n// useful tutorial: https://www.youtube.com/watch?v=CVClHLwv-4I&feature=youtu.be\r\n// https://github.com/WebDevSimplified/Face-Detection-JavaScript/blob/master/script.js\r\n\r\nimport * as faceapi from \"face-api.js\";\r\nimport { async } from \"q\";\r\nimport { setTimeout, setInterval } from \"timers\";\r\nimport FileSaver from \"file-saver\";\r\nimport { exists } from \"fs\";\r\nimport { shuffle } from \"@tensorflow/tfjs-core/dist/util\";\r\n\r\nexport default {\r\n  name: \"FaceDetection\",\r\n  components: {},\r\n  data: function() {\r\n    return {\r\n      display_size: { width: 640, height: 480 },\r\n      display_location: { top: 500, left: 50 },\r\n      detections: [],\r\n      resizedDetections: [],\r\n      refresh_time: 50,\r\n      // determine sensitivity for left and right face, higher = more turning needed\r\n      left_right_turning_ratio: 1.5,\r\n      // determine sensitivity for look up, higher = more turning needed\r\n      up_slope_threshold: -0.1,\r\n      // larger the more opening\r\n      mouth_threshold: 0.6,\r\n      // larger the more opening\r\n      eyebrow_threshold: 1.0,\r\n      // List of available instructions\r\n      instructions: [\r\n        \"Turn Left!\",\r\n        \"Turn Right!\",\r\n        \"Look Up!\",\r\n        \"Smile!\",\r\n        \"Open mouth!\",\r\n        \"Raise eyebrows!\"\r\n      ],\r\n      current_instruction: \"\",\r\n      liveness_detection_status: {\r\n        left_right_status: \"neutral\",\r\n        up_down_status: \"neutral\",\r\n        expression: \"neutral\",\r\n        mouth: \"neutral\",\r\n        eyebrow: \"neutral\"\r\n      },\r\n      liveness_test_instructions: []\r\n    };\r\n  },\r\n  mounted: function() {\r\n    this.load_models();\r\n    this.start_video();\r\n    this.generate_liveness_test();\r\n  },\r\n  computed: function(){\r\n\r\n  },\r\n  methods: {\r\n    generate_liveness_test: function() {\r\n      function shuffleArray(array) {\r\n        for (let i = array.length - 1; i > 0; i--) {\r\n          const j = Math.floor(Math.random() * (i + 1));\r\n          [array[i], array[j]] = [array[j], array[i]];\r\n        }\r\n      }\r\n      if (this.instructions.length == 0){\r\n        alert('you are a human')\r\n        this.current_instruction = 'do some humanitarian things!'\r\n      }\r\n      else{\r\n      shuffleArray(this.instructions);\r\n      this.current_instruction = this.instructions[0];}\r\n    },\r\n    liveness_test:function(){\r\n      var test = this.current_instruction\r\n      var result = this.liveness_detection_status\r\n      if (test == 'Turn Left!' && result.left_right_status =='left')\r\n      {return true}\r\n      else if (test == 'Turn Right!' && result.left_right_status =='right')\r\n      {return true}\r\n            else if (test == 'Look Up!' && result.up_down_status =='up')\r\n      {return true}\r\n          else if (test == 'Smile!' && result.expression =='happy')\r\n      {return true}\r\n                else if (test == 'Open mouth!' && result.mouth =='open')\r\n      {return true}\r\n                      else if (test == 'Raise eyebrows!' && result.eyebrow =='raised')\r\n      {return true}\r\n    },\r\n  \r\n    load_models: function() {\r\n      // save the models to the public/models\r\n      // faceapi.nets.ssdMobilenetv1.loadFromUri(\"/models\"),\r\n      Promise.all([\r\n        faceapi.nets.tinyFaceDetector.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceLandmark68Net.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceRecognitionNet.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceExpressionNet.loadFromUri(\"/models\")\r\n      ]).then(console.log(\"model loaded\"));\r\n    },\r\n    start_video: function() {\r\n      navigator.getUserMedia(\r\n        { video: {} },\r\n        stream => (video.srcObject = stream),\r\n        err => console.error(err)\r\n      );\r\n    },\r\n    detect_and_draw_faces: function() {\r\n      console.log(\"start detecting faces\");\r\n      // https://michaelnthiessen.com/this-is-undefined/\r\n      // https://stackoverflow.com/questions/47148363/when-to-use-vm-or-this-in-vue-js\r\n      // arrow function is a pain in vue.js and this async function also seems to cause problem\r\n      let self = this;\r\n\r\n      setInterval(async function() {\r\n        //  After face detection and facial landmark prediction the face descriptors\r\n        //  all the results are stored in self.detections\r\n        self.detections = await faceapi\r\n          .detectSingleFace(\r\n            self.$refs.video,\r\n            new faceapi.TinyFaceDetectorOptions()\r\n          )\r\n          .withFaceLandmarks()\r\n          .withFaceExpressions();\r\n        // calculating the 128 descriptors seems to be rather slow, lets not do it very frequent or do it in backend\r\n        // .withFaceDescriptors();\r\n\r\n        if (self.detections) {\r\n          // calculated the size that is on our canvas size\r\n          const resizedDetections = faceapi.resizeResults(\r\n            self.detections,\r\n            self.display_size\r\n          );\r\n\r\n          // replace the result with scaled one, all the other information will remain\r\n          self.detections = resizedDetections;\r\n\r\n          // call other methods\r\n          self.mouth_eye_status();\r\n          self.head_pose_estimation();\r\n          self.find_expression();\r\n\r\n          // annotation\r\n          var message2 = [\r\n            \"L/R: \" + self.liveness_detection_status.left_right_status,\r\n            \"U/D: \" + self.liveness_detection_status.up_down_status,\r\n            \"Expression: \" + self.liveness_detection_status.expression,\r\n            \"mouth: \" + self.liveness_detection_status.mouth,\r\n            \"eyebrow: \" + self.liveness_detection_status.eyebrow\r\n          ];\r\n          self.annotation({\r\n            clear: false,\r\n            message: \"HELLO\",\r\n            message2: message2\r\n          });\r\n\r\n          // liveness test is passed, we move on with next test\r\n          if (self.instructions.length != 0){\r\n          if (self.liveness_test()){\r\n              self.instructions.shift()\r\n              self.generate_liveness_test()\r\n              console.log('Passed one test, now move on!')\r\n          }\r\n          }\r\n\r\n        } else {\r\n          self.annotation({ clear: true });\r\n        }\r\n      }, self.refresh_time);\r\n    },\r\n    // note how to use named arguments in javascript...\r\n    annotation: function({ clear = false, message = \"Hello!\", message2 = [] }) {\r\n      var canvas = this.$refs.canvas;\r\n      var canvas_flip = this.$refs.canvas_flip;\r\n\r\n      // clear canvas before drawing the new things to prevent cluttering\r\n      canvas.getContext(\"2d\").clearRect(0, 0, canvas.width, canvas.height);\r\n      canvas_flip\r\n        .getContext(\"2d\")\r\n        .clearRect(0, 0, canvas_flip.width, canvas_flip.height);\r\n\r\n      // set to clear when no face can be found\r\n      if (clear === false) {\r\n        // NOTE: There are some useful drawing functions already exist\r\n        faceapi.draw.drawFaceLandmarks(canvas_flip, this.detections);\r\n\r\n        // Draw box around the face with 1 line text\r\n        const box = this.detections.detection.box;\r\n        // mirror X\r\n        const box_X_flipped = {\r\n          x: canvas.width - box.x - box.width,\r\n          y: box.y,\r\n          width: box.width,\r\n          height: box.height\r\n        };\r\n        // see DrawBoxOptions below\r\n        const boxOptions = {\r\n          label: message,\r\n          lineWidth: 2,\r\n          boxColor: \"rgba(57,255,20,0.8)\"\r\n        };\r\n        // flip the x-value x, y, width, height\r\n        const drawBox = new faceapi.draw.DrawBox(box_X_flipped, boxOptions);\r\n        drawBox.draw(canvas);\r\n\r\n        // ------- multi-line text at bottom -------\r\n        if (message2.length != 0) {\r\n          const text = message2;\r\n          const anchor = this.detections.detection.box.bottomLeft;\r\n          const anchor_X_flipped = {\r\n            x: canvas.width - anchor.x - box.width,\r\n            y: anchor.y\r\n          };\r\n          // see DrawTextField below\r\n          const textOptions = {\r\n            anchorPosition: \"TOP_LEFT\",\r\n            backgroundColor: \"rgba(57,255,20, 0.5)\"\r\n          };\r\n          const drawText = new faceapi.draw.DrawTextField(\r\n            text,\r\n            anchor_X_flipped,\r\n            textOptions\r\n          );\r\n          drawText.draw(canvas);\r\n        }\r\n      }\r\n    },\r\n    find_expression: function() {\r\n      var expressions = this.detections.expressions;\r\n      // find the max value of all the keys\r\n      // The reducer function takes four arguments: Accumulator (acc), Current Value (cur), Current Index (idx)\r\n      // the question mark is conditional operator, which returns true_val:false_val\r\n      var likely_expression = Object.keys(expressions).reduce((i, j) =>\r\n        expressions[i] > expressions[j] ? i : j\r\n      );\r\n      var expression_possibility = expressions[likely_expression];\r\n      this.liveness_detection_status[\"expression\"] = likely_expression;\r\n\r\n      return likely_expression + \":\" + String(expression_possibility);\r\n    },\r\n    head_pose_estimation: function() {\r\n      const landmarks = this.detections.landmarks;\r\n      const landmarkPositions = landmarks.positions;\r\n\r\n      // the distance between 1 and 28 will be shorter when extending left face\r\n      const right_face_distance = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[16]),\r\n        Object.values(landmarkPositions[28])\r\n      );\r\n      const left_face_distance = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[0]),\r\n        Object.values(landmarkPositions[28])\r\n      );\r\n      const right_to_left_face_ratio = right_face_distance / left_face_distance;\r\n\r\n      // the point 6 - 12 become more flat when facing upward. calculate the slope\r\n      const chin_slope_left =\r\n        (landmarkPositions[5].y - landmarkPositions[8].y) /\r\n        (landmarkPositions[8].x - landmarkPositions[5].x);\r\n      const chin_slope_right =\r\n        (landmarkPositions[11].y - landmarkPositions[8].y) /\r\n        (landmarkPositions[11].x - landmarkPositions[8].x);\r\n      const chin_slope_average = (chin_slope_left + chin_slope_right) / 2;\r\n      // NOTE: make this vue variable\r\n\r\n      if (\r\n        right_to_left_face_ratio > 1 / this.left_right_turning_ratio &&\r\n        right_to_left_face_ratio < this.left_right_turning_ratio\r\n      ) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"neutral\";\r\n      } else if (right_to_left_face_ratio > this.left_right_turning_ratio) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"right\";\r\n      } else if (right_to_left_face_ratio < 1 / this.left_right_turning_ratio) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"left\";\r\n      }\r\n      if (chin_slope_average > this.up_slope_threshold) {\r\n        this.liveness_detection_status[\"up_down_status\"] = \"up\";\r\n      } else {\r\n        this.liveness_detection_status[\"up_down_status\"] = \"neutral\";\r\n      }\r\n    },\r\n    mouth_eye_status: function() {\r\n      //  https://towardsdatascience.com/mouse-control-facial-movements-hci-app-c16b0494a971\r\n      var landmarks = this.detections.landmarks;\r\n      const landmarkPositions = landmarks.positions;\r\n      // https://www.pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/\r\n      const mouth_y = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[51]),\r\n        Object.values(landmarkPositions[57])\r\n      );\r\n      const mouth_x = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[54]),\r\n        Object.values(landmarkPositions[48])\r\n      );\r\n      const mouth_aspect_ratio = mouth_y / mouth_x;\r\n      if (mouth_aspect_ratio > this.mouth_threshold){\r\n        this.liveness_detection_status.mouth = 'open'\r\n      }\r\n      else {\r\n        this.liveness_detection_status.mouth = 'neutral'\r\n      }\r\n\r\n      //  ------------- eyebrow measurment: use the distance between eyebrow and eye vs \r\n      // the width of the eyes (rather constant)  to test whether the eyebrows are raised\r\n      const l_eyebrow_eye_d = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[19]),\r\n        Object.values(landmarkPositions[37])\r\n      );\r\n      const r_eyebrow_eye_d = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[24]),\r\n        Object.values(landmarkPositions[44])\r\n      );\r\n      // use eyebrow as a reference point\r\n      const l_eye_w = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[39]),\r\n        Object.values(landmarkPositions[36])\r\n      );\r\n      const r_eye_w = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[42]),\r\n        Object.values(landmarkPositions[45])\r\n      );\r\n      const eyebrow_d_eye_w_ratio =\r\n        (l_eyebrow_eye_d + r_eyebrow_eye_d) / (l_eye_w + r_eye_w);\r\n\r\n      if (eyebrow_d_eye_w_ratio > this.eyebrow_threshold){\r\n        this.liveness_detection_status.eyebrow = 'raised'\r\n      }\r\n      else {\r\n        this.liveness_detection_status.eyebrow = 'neutral'\r\n      }\r\n\r\n    },\r\n    download_face_descriptor: function() {\r\n      console.log(this.detections);\r\n      var detections_json = JSON.stringify(this.detections);\r\n      var blob = new Blob([detections_json], { type: \"application/json\" });\r\n      FileSaver.saveAs(blob, \"key.json\");\r\n    },\r\n    communicate_face_descriptor: function() {\r\n      console.log(this.detections);\r\n      // specify which person as the algo track all the faces\r\n      console.log(this.detections[\"0\"].descriptor);\r\n      this.axios({\r\n        url: this.$API_URL + \"/face_descriptor\",\r\n        method: \"POST\",\r\n        data: {\r\n          descriptor: this.detections[\"0\"].descriptor\r\n        }\r\n      }).then(response => {});\r\n    }\r\n  }\r\n};\r\n",{"version":3,"sources":["FaceDetection.vue"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA8CA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","file":"FaceDetection.vue","sourceRoot":"src/components","sourcesContent":["<template>\r\n  <div>\r\n    <v-container grid-list-md text-xs-center fluid>\r\n      <v-layout justify-center>\r\n        <label>Instruction: {{current_instruction}}</label>\r\n      </v-layout>\r\n      <v-layout justify-center>\r\n        <video\r\n          id=\"video\"\r\n          ref=\"video\"\r\n          :width=\"display_size.width\"\r\n          :height=\"display_size.height\"\r\n          :top=\"display_location.top\"\r\n          :left=\"display_location.left\"\r\n          style=\"position: absolute;\"\r\n          autoplay\r\n          @play=\"detect_and_draw_faces\"\r\n        ></video>\r\n\r\n        <canvas\r\n          id=\"canvas_flip\"\r\n          ref=\"canvas_flip\"\r\n          :width=\"display_size.width\"\r\n          :height=\"display_size.height\"\r\n          :top=\"display_location.top\"\r\n          :left=\"display_location.left\"\r\n          style=\"transform: scaleX(-1);\"\r\n        ></canvas>\r\n\r\n        <canvas\r\n          id=\"canvas\"\r\n          ref=\"canvas\"\r\n          :width=\"display_size.width\"\r\n          :height=\"display_size.height\"\r\n          :top=\"display_location.top\"\r\n          :left=\"display_location.left\"\r\n        ></canvas>\r\n      </v-layout>\r\n      <v-layout align-center justify-center>\r\n        <v-btn @click=\"download_face_descriptor\">download vector</v-btn>\r\n      </v-layout>\r\n    </v-container>\r\n  </div>\r\n</template>\r\n\r\n<script>\r\n// useful tutorial: https://www.youtube.com/watch?v=CVClHLwv-4I&feature=youtu.be\r\n// https://github.com/WebDevSimplified/Face-Detection-JavaScript/blob/master/script.js\r\n\r\nimport * as faceapi from \"face-api.js\";\r\nimport { async } from \"q\";\r\nimport { setTimeout, setInterval } from \"timers\";\r\nimport FileSaver from \"file-saver\";\r\nimport { exists } from \"fs\";\r\nimport { shuffle } from \"@tensorflow/tfjs-core/dist/util\";\r\n\r\nexport default {\r\n  name: \"FaceDetection\",\r\n  components: {},\r\n  data: function() {\r\n    return {\r\n      display_size: { width: 640, height: 480 },\r\n      display_location: { top: 500, left: 50 },\r\n      detections: [],\r\n      resizedDetections: [],\r\n      refresh_time: 50,\r\n      // determine sensitivity for left and right face, higher = more turning needed\r\n      left_right_turning_ratio: 1.5,\r\n      // determine sensitivity for look up, higher = more turning needed\r\n      up_slope_threshold: -0.1,\r\n      // larger the more opening\r\n      mouth_threshold: 0.6,\r\n      // larger the more opening\r\n      eyebrow_threshold: 1.0,\r\n      // List of available instructions\r\n      instructions: [\r\n        \"Turn Left!\",\r\n        \"Turn Right!\",\r\n        \"Look Up!\",\r\n        \"Smile!\",\r\n        \"Open mouth!\",\r\n        \"Raise eyebrows!\"\r\n      ],\r\n      current_instruction: \"\",\r\n      liveness_detection_status: {\r\n        left_right_status: \"neutral\",\r\n        up_down_status: \"neutral\",\r\n        expression: \"neutral\",\r\n        mouth: \"neutral\",\r\n        eyebrow: \"neutral\"\r\n      },\r\n      liveness_test_instructions: []\r\n    };\r\n  },\r\n  mounted: function() {\r\n    this.load_models();\r\n    this.start_video();\r\n    this.generate_liveness_test();\r\n  },\r\n  computed: function(){\r\n\r\n  },\r\n  methods: {\r\n    generate_liveness_test: function() {\r\n      function shuffleArray(array) {\r\n        for (let i = array.length - 1; i > 0; i--) {\r\n          const j = Math.floor(Math.random() * (i + 1));\r\n          [array[i], array[j]] = [array[j], array[i]];\r\n        }\r\n      }\r\n      if (this.instructions.length == 0){\r\n        alert('you are a human')\r\n        this.current_instruction = 'do some humanitarian things!'\r\n      }\r\n      else{\r\n      shuffleArray(this.instructions);\r\n      this.current_instruction = this.instructions[0];}\r\n    },\r\n    liveness_test:function(){\r\n      var test = this.current_instruction\r\n      var result = this.liveness_detection_status\r\n      if (test == 'Turn Left!' && result.left_right_status =='left')\r\n      {return true}\r\n      else if (test == 'Turn Right!' && result.left_right_status =='right')\r\n      {return true}\r\n            else if (test == 'Look Up!' && result.up_down_status =='up')\r\n      {return true}\r\n          else if (test == 'Smile!' && result.expression =='happy')\r\n      {return true}\r\n                else if (test == 'Open mouth!' && result.mouth =='open')\r\n      {return true}\r\n                      else if (test == 'Raise eyebrows!' && result.eyebrow =='raised')\r\n      {return true}\r\n    },\r\n  \r\n    load_models: function() {\r\n      // save the models to the public/models\r\n      // faceapi.nets.ssdMobilenetv1.loadFromUri(\"/models\"),\r\n      Promise.all([\r\n        faceapi.nets.tinyFaceDetector.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceLandmark68Net.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceRecognitionNet.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceExpressionNet.loadFromUri(\"/models\")\r\n      ]).then(console.log(\"model loaded\"));\r\n    },\r\n    start_video: function() {\r\n      navigator.getUserMedia(\r\n        { video: {} },\r\n        stream => (video.srcObject = stream),\r\n        err => console.error(err)\r\n      );\r\n    },\r\n    detect_and_draw_faces: function() {\r\n      console.log(\"start detecting faces\");\r\n      // https://michaelnthiessen.com/this-is-undefined/\r\n      // https://stackoverflow.com/questions/47148363/when-to-use-vm-or-this-in-vue-js\r\n      // arrow function is a pain in vue.js and this async function also seems to cause problem\r\n      let self = this;\r\n\r\n      setInterval(async function() {\r\n        //  After face detection and facial landmark prediction the face descriptors\r\n        //  all the results are stored in self.detections\r\n        self.detections = await faceapi\r\n          .detectSingleFace(\r\n            self.$refs.video,\r\n            new faceapi.TinyFaceDetectorOptions()\r\n          )\r\n          .withFaceLandmarks()\r\n          .withFaceExpressions();\r\n        // calculating the 128 descriptors seems to be rather slow, lets not do it very frequent or do it in backend\r\n        // .withFaceDescriptors();\r\n\r\n        if (self.detections) {\r\n          // calculated the size that is on our canvas size\r\n          const resizedDetections = faceapi.resizeResults(\r\n            self.detections,\r\n            self.display_size\r\n          );\r\n\r\n          // replace the result with scaled one, all the other information will remain\r\n          self.detections = resizedDetections;\r\n\r\n          // call other methods\r\n          self.mouth_eye_status();\r\n          self.head_pose_estimation();\r\n          self.find_expression();\r\n\r\n          // annotation\r\n          var message2 = [\r\n            \"L/R: \" + self.liveness_detection_status.left_right_status,\r\n            \"U/D: \" + self.liveness_detection_status.up_down_status,\r\n            \"Expression: \" + self.liveness_detection_status.expression,\r\n            \"mouth: \" + self.liveness_detection_status.mouth,\r\n            \"eyebrow: \" + self.liveness_detection_status.eyebrow\r\n          ];\r\n          self.annotation({\r\n            clear: false,\r\n            message: \"HELLO\",\r\n            message2: message2\r\n          });\r\n\r\n          // liveness test is passed, we move on with next test\r\n          if (self.instructions.length != 0){\r\n          if (self.liveness_test()){\r\n              self.instructions.shift()\r\n              self.generate_liveness_test()\r\n              console.log('Passed one test, now move on!')\r\n          }\r\n          }\r\n\r\n        } else {\r\n          self.annotation({ clear: true });\r\n        }\r\n      }, self.refresh_time);\r\n    },\r\n    // note how to use named arguments in javascript...\r\n    annotation: function({ clear = false, message = \"Hello!\", message2 = [] }) {\r\n      var canvas = this.$refs.canvas;\r\n      var canvas_flip = this.$refs.canvas_flip;\r\n\r\n      // clear canvas before drawing the new things to prevent cluttering\r\n      canvas.getContext(\"2d\").clearRect(0, 0, canvas.width, canvas.height);\r\n      canvas_flip\r\n        .getContext(\"2d\")\r\n        .clearRect(0, 0, canvas_flip.width, canvas_flip.height);\r\n\r\n      // set to clear when no face can be found\r\n      if (clear === false) {\r\n        // NOTE: There are some useful drawing functions already exist\r\n        faceapi.draw.drawFaceLandmarks(canvas_flip, this.detections);\r\n\r\n        // Draw box around the face with 1 line text\r\n        const box = this.detections.detection.box;\r\n        // mirror X\r\n        const box_X_flipped = {\r\n          x: canvas.width - box.x - box.width,\r\n          y: box.y,\r\n          width: box.width,\r\n          height: box.height\r\n        };\r\n        // see DrawBoxOptions below\r\n        const boxOptions = {\r\n          label: message,\r\n          lineWidth: 2,\r\n          boxColor: \"rgba(57,255,20,0.8)\"\r\n        };\r\n        // flip the x-value x, y, width, height\r\n        const drawBox = new faceapi.draw.DrawBox(box_X_flipped, boxOptions);\r\n        drawBox.draw(canvas);\r\n\r\n        // ------- multi-line text at bottom -------\r\n        if (message2.length != 0) {\r\n          const text = message2;\r\n          const anchor = this.detections.detection.box.bottomLeft;\r\n          const anchor_X_flipped = {\r\n            x: canvas.width - anchor.x - box.width,\r\n            y: anchor.y\r\n          };\r\n          // see DrawTextField below\r\n          const textOptions = {\r\n            anchorPosition: \"TOP_LEFT\",\r\n            backgroundColor: \"rgba(57,255,20, 0.5)\"\r\n          };\r\n          const drawText = new faceapi.draw.DrawTextField(\r\n            text,\r\n            anchor_X_flipped,\r\n            textOptions\r\n          );\r\n          drawText.draw(canvas);\r\n        }\r\n      }\r\n    },\r\n    find_expression: function() {\r\n      var expressions = this.detections.expressions;\r\n      // find the max value of all the keys\r\n      // The reducer function takes four arguments: Accumulator (acc), Current Value (cur), Current Index (idx)\r\n      // the question mark is conditional operator, which returns true_val:false_val\r\n      var likely_expression = Object.keys(expressions).reduce((i, j) =>\r\n        expressions[i] > expressions[j] ? i : j\r\n      );\r\n      var expression_possibility = expressions[likely_expression];\r\n      this.liveness_detection_status[\"expression\"] = likely_expression;\r\n\r\n      return likely_expression + \":\" + String(expression_possibility);\r\n    },\r\n    head_pose_estimation: function() {\r\n      const landmarks = this.detections.landmarks;\r\n      const landmarkPositions = landmarks.positions;\r\n\r\n      // the distance between 1 and 28 will be shorter when extending left face\r\n      const right_face_distance = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[16]),\r\n        Object.values(landmarkPositions[28])\r\n      );\r\n      const left_face_distance = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[0]),\r\n        Object.values(landmarkPositions[28])\r\n      );\r\n      const right_to_left_face_ratio = right_face_distance / left_face_distance;\r\n\r\n      // the point 6 - 12 become more flat when facing upward. calculate the slope\r\n      const chin_slope_left =\r\n        (landmarkPositions[5].y - landmarkPositions[8].y) /\r\n        (landmarkPositions[8].x - landmarkPositions[5].x);\r\n      const chin_slope_right =\r\n        (landmarkPositions[11].y - landmarkPositions[8].y) /\r\n        (landmarkPositions[11].x - landmarkPositions[8].x);\r\n      const chin_slope_average = (chin_slope_left + chin_slope_right) / 2;\r\n      // NOTE: make this vue variable\r\n\r\n      if (\r\n        right_to_left_face_ratio > 1 / this.left_right_turning_ratio &&\r\n        right_to_left_face_ratio < this.left_right_turning_ratio\r\n      ) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"neutral\";\r\n      } else if (right_to_left_face_ratio > this.left_right_turning_ratio) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"right\";\r\n      } else if (right_to_left_face_ratio < 1 / this.left_right_turning_ratio) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"left\";\r\n      }\r\n      if (chin_slope_average > this.up_slope_threshold) {\r\n        this.liveness_detection_status[\"up_down_status\"] = \"up\";\r\n      } else {\r\n        this.liveness_detection_status[\"up_down_status\"] = \"neutral\";\r\n      }\r\n    },\r\n    mouth_eye_status: function() {\r\n      //  https://towardsdatascience.com/mouse-control-facial-movements-hci-app-c16b0494a971\r\n      var landmarks = this.detections.landmarks;\r\n      const landmarkPositions = landmarks.positions;\r\n      // https://www.pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/\r\n      const mouth_y = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[51]),\r\n        Object.values(landmarkPositions[57])\r\n      );\r\n      const mouth_x = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[54]),\r\n        Object.values(landmarkPositions[48])\r\n      );\r\n      const mouth_aspect_ratio = mouth_y / mouth_x;\r\n      if (mouth_aspect_ratio > this.mouth_threshold){\r\n        this.liveness_detection_status.mouth = 'open'\r\n      }\r\n      else {\r\n        this.liveness_detection_status.mouth = 'neutral'\r\n      }\r\n\r\n      //  ------------- eyebrow measurment: use the distance between eyebrow and eye vs \r\n      // the width of the eyes (rather constant)  to test whether the eyebrows are raised\r\n      const l_eyebrow_eye_d = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[19]),\r\n        Object.values(landmarkPositions[37])\r\n      );\r\n      const r_eyebrow_eye_d = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[24]),\r\n        Object.values(landmarkPositions[44])\r\n      );\r\n      // use eyebrow as a reference point\r\n      const l_eye_w = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[39]),\r\n        Object.values(landmarkPositions[36])\r\n      );\r\n      const r_eye_w = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[42]),\r\n        Object.values(landmarkPositions[45])\r\n      );\r\n      const eyebrow_d_eye_w_ratio =\r\n        (l_eyebrow_eye_d + r_eyebrow_eye_d) / (l_eye_w + r_eye_w);\r\n\r\n      if (eyebrow_d_eye_w_ratio > this.eyebrow_threshold){\r\n        this.liveness_detection_status.eyebrow = 'raised'\r\n      }\r\n      else {\r\n        this.liveness_detection_status.eyebrow = 'neutral'\r\n      }\r\n\r\n    },\r\n    download_face_descriptor: function() {\r\n      console.log(this.detections);\r\n      var detections_json = JSON.stringify(this.detections);\r\n      var blob = new Blob([detections_json], { type: \"application/json\" });\r\n      FileSaver.saveAs(blob, \"key.json\");\r\n    },\r\n    communicate_face_descriptor: function() {\r\n      console.log(this.detections);\r\n      // specify which person as the algo track all the faces\r\n      console.log(this.detections[\"0\"].descriptor);\r\n      this.axios({\r\n        url: this.$API_URL + \"/face_descriptor\",\r\n        method: \"POST\",\r\n        data: {\r\n          descriptor: this.detections[\"0\"].descriptor\r\n        }\r\n      }).then(response => {});\r\n    }\r\n  }\r\n};\r\n</script>\r\n\r\n<style scope>\r\n@import url(\"https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css\");\r\n\r\n#video {\r\n  /* position: absolute; */\r\n  pointer-events: none;\r\n  /* flip the image so it is more natural */\r\n  transform: scaleX(-1);\r\n}\r\n\r\n#canvas {\r\n  position: absolute;\r\n  pointer-events: none;\r\n}\r\n\r\n.dropzone-custom-title {\r\n  margin-top: 0;\r\n  color: #00b782;\r\n}\r\n\r\n.subtitle {\r\n  color: #314b5f;\r\n}\r\n</style>\r\n"]}]}