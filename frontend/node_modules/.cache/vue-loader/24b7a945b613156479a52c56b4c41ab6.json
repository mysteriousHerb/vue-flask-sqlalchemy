{"remainingRequest":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\node_modules\\vue-loader\\lib\\index.js??vue-loader-options!C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\src\\components\\FaceDetection.vue?vue&type=script&lang=js&","dependencies":[{"path":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\src\\components\\FaceDetection.vue","mtime":1561411170817},{"path":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\node_modules\\cache-loader\\dist\\cjs.js","mtime":1560457563834},{"path":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\node_modules\\babel-loader\\lib\\index.js","mtime":1560457563473},{"path":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\node_modules\\cache-loader\\dist\\cjs.js","mtime":1560457563834},{"path":"C:\\Users\\My Pc\\Documents\\GitHub\\vue-flask-sqlalchemy\\frontend\\node_modules\\vue-loader\\lib\\index.js","mtime":1560457580480}],"contextDependencies":[],"result":["//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n//\n\r\n// useful tutorial: https://www.youtube.com/watch?v=CVClHLwv-4I&feature=youtu.be\r\n// https://github.com/WebDevSimplified/Face-Detection-JavaScript/blob/master/script.js\r\n\r\nimport * as faceapi from \"face-api.js\";\r\nimport { async } from \"q\";\r\nimport { setTimeout, setInterval } from \"timers\";\r\nimport FileSaver from \"file-saver\";\r\nimport Icon from \"vue-awesome/components/Icon\";\r\nimport vue2Dropzone from \"vue2-dropzone\";\r\nimport \"vue2-dropzone/dist/vue2Dropzone.min.css\";\r\n\r\nexport default {\r\n  name: \"FaceDetection\",\r\n  components: {\r\n    \"v-fa-icon\": Icon,\r\n    vueDropzone: vue2Dropzone\r\n  },\r\n  data: function() {\r\n    return {\r\n      display_size: { width: 640, height: 480 },\r\n      detections: [],\r\n      temp_detections: [],\r\n      capture_file_count: 0,\r\n      detected: false,\r\n      resizedDetections: [],\r\n      confirmed_user: \"\",\r\n      refresh_time: 50,\r\n      // determine sensitivity for left and right face, higher = more turning needed\r\n      left_right_turning_ratio: 2,\r\n      // determine sensitivity for look up, higher = more turning needed\r\n      up_slope_threshold: -0.1,\r\n      down_slope_threshold: -0.55,\r\n      // larger the more opening\r\n      mouth_threshold: 0.7,\r\n      // larger the more opening\r\n      eyebrow_threshold: 1.0,\r\n      // List of available instructions\r\n      instructions: [\r\n        { command: \"Turn Left!\", icon: \"arrow-alt-circle-left\" },\r\n        { command: \"Turn Right!\", icon: \"arrow-alt-circle-right\" },\r\n        { command: \"Look Up!\", icon: \"arrow-alt-circle-up\" },\r\n        { command: \"Look Down!\", icon: \"arrow-alt-circle-down\" },\r\n        { command: \"Smile!\", icon: \"laugh\" },\r\n        { command: \"Open mouth!\", icon: \"teeth-open\" }\r\n\r\n        // \"Raise eyebrows!\"\r\n      ],\r\n      current_instruction: { command: \"\", icon: \"\" },\r\n      liveness_detection_status: {\r\n        left_right_status: \"neutral\",\r\n        up_down_status: \"neutral\",\r\n        expression: \"neutral\",\r\n        mouth: \"neutral\",\r\n        eyebrow: \"neutral\"\r\n      },\r\n      liveness_test_instructions: []\r\n    };\r\n  },\r\n  watch: {\r\n    temp_detections: function(new_val, old_val) {\r\n      console.log(new_val);\r\n    }\r\n  },\r\n  mounted: function() {\r\n    this.load_faceapi_models();\r\n    this.start_video();\r\n    this.generate_liveness_test();\r\n  },\r\n  computed: {\r\n    session_id() {\r\n      // session_id should be sent to all the axios request to backend now\r\n      return this.$store.state.session_id;\r\n    },\r\n      user_name_in_key(){\r\n      return this.$store.state.user_name_in_key;\r\n    }\r\n  },\r\n  methods: {\r\n    generate_liveness_test: function() {\r\n      // DEBUG: add a timer for each test?\r\n      function shuffleArray(array) {\r\n        for (let i = array.length - 1; i > 0; i--) {\r\n          const j = Math.floor(Math.random() * (i + 1));\r\n          [array[i], array[j]] = [array[j], array[i]];\r\n        }\r\n      }\r\n      if (this.instructions.length == 0) {\r\n        console.log(\"you are a human!\");\r\n        this.current_instruction.command = \"\";\r\n        this.match_known_descriptor();\r\n      } else {\r\n        shuffleArray(this.instructions);\r\n        this.current_instruction = this.instructions[0];\r\n      }\r\n    },\r\n    liveness_test: function() {\r\n      var test = this.current_instruction.command;\r\n      var result = this.liveness_detection_status;\r\n      // use the neutral pose to ensure we don't get false detection\r\n      var neutral_l_r_pose = result.left_right_status == \"neutral\";\r\n      var neutral_u_d_pose = result.up_down_status == \"neutral\";\r\n      var neutral_mouth = result.mouth == \"neutral\";\r\n\r\n      if (test == \"Turn Left!\" && result.left_right_status == \"left\") {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      } else if (test == \"Turn Right!\" && result.left_right_status == \"right\") {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      } else if (\r\n        test == \"Look Up!\" &&\r\n        result.up_down_status == \"up\" &&\r\n        neutral_l_r_pose &&\r\n        neutral_mouth\r\n      ) {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      } else if (\r\n        test == \"Look Down!\" &&\r\n        result.up_down_status == \"down\" &&\r\n        neutral_l_r_pose &&\r\n        neutral_mouth\r\n      ) {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      } else if (\r\n        test == \"Smile!\" &&\r\n        result.expression == \"happy\" &&\r\n        neutral_l_r_pose &&\r\n        neutral_u_d_pose\r\n      ) {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      } else if (\r\n        test == \"Open mouth!\" &&\r\n        result.mouth == \"open\" &&\r\n        neutral_l_r_pose\r\n      ) {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      }\r\n      // } else if (\r\n      //   test == \"Raise eyebrows!\" &&\r\n      //   result.eyebrow == \"raised\" &&\r\n      //   neutral_l_r_pose &&\r\n      //   neutral_u_d_pose\r\n      // ) {\r\n      //   return true;\r\n      // }\r\n    },\r\n\r\n    load_faceapi_models: function() {\r\n      // save the models to the public/models\r\n      // faceapi.nets.ssdMobilenetv1.loadFromUri(\"/models\"),\r\n      Promise.all([\r\n        faceapi.nets.tinyFaceDetector.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceLandmark68Net.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceRecognitionNet.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceExpressionNet.loadFromUri(\"/models\")\r\n      ]).then(console.log(\"model loaded\"));\r\n    },\r\n    start_video: function() {\r\n      navigator.getUserMedia(\r\n        { video: {} },\r\n        stream => (video.srcObject = stream),\r\n        err => console.error(err)\r\n      );\r\n    },\r\n    generate_face_descriptor: function() {\r\n      // https://x-team.com/blog/webcam-capture-vue/\r\n      let self = this;\r\n      if (self.detections) {\r\n        // keep track how many images we have captured\r\n        self.capture_file_count += 1;\r\n        // console.log(this.detections);\r\n        console.log(self.capture_file_count);\r\n        const canvas = self.$refs.canvas_capture;\r\n        var context = canvas.getContext(\"2d\");\r\n        context.clearRect(0, 0, canvas.width, canvas.height);\r\n        var box = self.detections.detection.box;\r\n        // giving some padding so we dont crop too much which gives problem to backend\r\n        console.log(box)\r\n        console.log([canvas.width, canvas.height])\r\n\r\n\r\n        // Sending the face with cropped region \r\n        // drawImage from the video stream, with cropping\r\n        context.drawImage(\r\n          self.$refs.video,\r\n          box.x,\r\n          box.y,\r\n          box.width,\r\n          box.height,\r\n          box.x,\r\n          box.y,\r\n          box.width,\r\n          box.height\r\n        );\r\n\r\n        // // sending the whole image \r\n        //   context.drawImage(\r\n        //   self.$refs.video,\r\n        //   0, 0, self.display_size['width'], self.display_size['height']\r\n        // );\r\n\r\n        // // // sending the face_location to backend to save some repetition \r\n        // // // top, right, bottom, left\r\n        const face_location = [box.top, box.right, box.bottom, box.left]\r\n\r\n\r\n\r\n\r\n        // Saving canvas to local drive is easy: https://github.com/eligrey/FileSaver.js/\r\n        self.$refs.canvas_capture.toBlob(async function(blob) {\r\n          // result = await faceapi.bufferToImage(blob).\r\n          // upload Blob as a form to the flask backend and generate descriptors\r\n          // https://github.com/pagekit/vue-resource/blob/master/docs/recipes.md\r\n          let formData = new FormData();\r\n          // formData.append(name, value, filename);\r\n          formData.append(\r\n            \"file\",\r\n            blob,\r\n            \"unknown_face_\" + self.capture_file_count + \".jpg\"\r\n          );\r\n          formData.append(\"session_id\", self.session_id);\r\n          formData.append(\"face_location\", face_location);\r\n          self.axios({\r\n            url: self.$API_URL + \"/generate_descriptor\",\r\n            method: \"POST\",\r\n            data: formData\r\n          });\r\n        });\r\n      }\r\n    },\r\n    match_known_descriptor: function() {\r\n      this.axios({\r\n        url: this.$API_URL + \"/compare_descriptors\",\r\n        method: \"POST\",\r\n        data: {session_id: this.session_id}\r\n      }).then(response => {\r\n        if (response.data.match) {\r\n          this.confirmed_user = response.data.user;\r\n        }\r\n      });\r\n    },\r\n    show_capture: function() {\r\n      console.log(this.temp_detections);\r\n    },\r\n    detect_and_draw_faces: function() {\r\n      console.log(\"start detecting faces\");\r\n      // https://michaelnthiessen.com/this-is-undefined/\r\n      // https://stackoverflow.com/questions/47148363/when-to-use-vm-or-this-in-vue-js\r\n      // arrow function is a pain in vue.js and this async function also seems to cause problem\r\n      let self = this;\r\n\r\n      setInterval(async function() {\r\n        //  After face detection and facial landmark prediction the face descriptors\r\n        //  all the results are stored in self.detections\r\n        self.detections = await faceapi\r\n          .detectSingleFace(\r\n            self.$refs.video,\r\n            new faceapi.TinyFaceDetectorOptions()\r\n          )\r\n          .withFaceLandmarks()\r\n          .withFaceExpressions();\r\n        // calculating the 128 descriptors seems to be rather slow, lets not do it very frequent or do it in backend\r\n        // .withFaceDescriptors();\r\n\r\n        if (self.detections) {\r\n          self.detected = true;\r\n          // calculated the size that is on our canvas size\r\n          const resizedDetections = faceapi.resizeResults(\r\n            self.detections,\r\n            self.display_size\r\n          );\r\n\r\n          // replace the result with scaled one, all the other information will remain\r\n          self.detections = resizedDetections;\r\n\r\n          // call other methods\r\n          self.mouth_eye_status();\r\n          self.head_pose_estimation();\r\n          self.find_expression();\r\n\r\n          // annotation\r\n          var message2 = [\r\n            \"L/R: \" + self.liveness_detection_status.left_right_status,\r\n            \"U/D: \" + self.liveness_detection_status.up_down_status,\r\n            \"Expression: \" + self.liveness_detection_status.expression,\r\n            \"mouth: \" + self.liveness_detection_status.mouth,\r\n            \"eyebrow: \" + self.liveness_detection_status.eyebrow\r\n          ];\r\n          var message = \"Hello\";\r\n          // only append the username after liveness test\r\n          if (self.instructions.length == 0 && self.confirmed_user != \"\") {\r\n            message = \"Hello: \" + self.user_name_in_key + \"!\";\r\n          } else if (\r\n            self.instructions.length == 0 &&\r\n            self.confirmed_user == \"\"\r\n          ) {\r\n            message = \"Hello: Stranger! Maybe you are not who you claim to be\";\r\n          }\r\n\r\n          self.annotation({\r\n            clear: false,\r\n            message: message,\r\n            message2: message2\r\n          });\r\n\r\n          // liveness test is passed, we move on with next test\r\n          if (self.instructions.length != 0) {\r\n            if (self.liveness_test()) {\r\n              self.instructions.shift();\r\n              self.generate_liveness_test();\r\n              console.log(\"Passed one test, now move on!\");\r\n            }\r\n          }\r\n        } else {\r\n          self.annotation({ clear: true });\r\n        }\r\n      }, self.refresh_time);\r\n    },\r\n    // note how to use named arguments in javascript...\r\n    annotation: function({ clear = false, message = \"Hello!\", message2 = [] }) {\r\n      var canvas = this.$refs.canvas;\r\n      var canvas_flip = this.$refs.canvas_flip;\r\n\r\n      // clear canvas before drawing the new things to prevent cluttering\r\n      canvas.getContext(\"2d\").clearRect(0, 0, canvas.width, canvas.height);\r\n      canvas_flip\r\n        .getContext(\"2d\")\r\n        .clearRect(0, 0, canvas_flip.width, canvas_flip.height);\r\n\r\n      // set to clear when no face can be found\r\n      if (clear === false) {\r\n        // NOTE: There are some useful drawing functions already exist\r\n        faceapi.draw.drawFaceLandmarks(canvas_flip, this.detections);\r\n\r\n        // Draw box around the face with 1 line text\r\n        const box = this.detections.detection.box;\r\n        // mirror X\r\n        const box_X_flipped = {\r\n          x: canvas.width - box.x - box.width,\r\n          y: box.y,\r\n          width: box.width,\r\n          height: box.height\r\n        };\r\n        // see DrawBoxOptions below\r\n        const boxOptions = {\r\n          label: message,\r\n          lineWidth: 2,\r\n          boxColor: \"rgba(57,255,20,0.8)\"\r\n        };\r\n        // flip the x-value x, y, width, height\r\n        const drawBox = new faceapi.draw.DrawBox(box_X_flipped, boxOptions);\r\n        drawBox.draw(canvas);\r\n\r\n        // ------- multi-line text at bottom -------\r\n        if (message2.length != 0) {\r\n          const text = message2;\r\n          const anchor = this.detections.detection.box.bottomLeft;\r\n          const anchor_X_flipped = {\r\n            x: canvas.width - anchor.x - box.width,\r\n            y: anchor.y\r\n          };\r\n          // see DrawTextField below\r\n          const textOptions = {\r\n            anchorPosition: \"TOP_LEFT\",\r\n            backgroundColor: \"rgba(57,255,20, 0.5)\"\r\n          };\r\n          const drawText = new faceapi.draw.DrawTextField(\r\n            text,\r\n            anchor_X_flipped,\r\n            textOptions\r\n          );\r\n          drawText.draw(canvas);\r\n        }\r\n      }\r\n    },\r\n    find_expression: function() {\r\n      var expressions = this.detections.expressions;\r\n      // find the max value of all the keys\r\n      // The reducer function takes four arguments: Accumulator (acc), Current Value (cur), Current Index (idx)\r\n      // the question mark is conditional operator, which returns true_val:false_val\r\n      var likely_expression = Object.keys(expressions).reduce((i, j) =>\r\n        expressions[i] > expressions[j] ? i : j\r\n      );\r\n      var expression_possibility = expressions[likely_expression];\r\n      this.liveness_detection_status[\"expression\"] = likely_expression;\r\n\r\n      return likely_expression + \":\" + String(expression_possibility);\r\n    },\r\n    head_pose_estimation: function() {\r\n      const landmarks = this.detections.landmarks;\r\n      const landmarkPositions = landmarks.positions;\r\n\r\n      // the distance between 1 and 28 will be shorter when extending left face\r\n      const right_face_distance = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[16]),\r\n        Object.values(landmarkPositions[28])\r\n      );\r\n      const left_face_distance = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[0]),\r\n        Object.values(landmarkPositions[28])\r\n      );\r\n      const right_to_left_face_ratio = right_face_distance / left_face_distance;\r\n\r\n      // the point 6 - 12 become more flat when facing upward. calculate the slope\r\n      const chin_slope_left =\r\n        (landmarkPositions[5].y - landmarkPositions[8].y) /\r\n        (landmarkPositions[8].x - landmarkPositions[5].x);\r\n      const chin_slope_right =\r\n        (landmarkPositions[11].y - landmarkPositions[8].y) /\r\n        (landmarkPositions[11].x - landmarkPositions[8].x);\r\n      const chin_slope_average = (chin_slope_left + chin_slope_right) / 2;\r\n      // NOTE: make this vue variable\r\n\r\n      if (\r\n        right_to_left_face_ratio > 1 / this.left_right_turning_ratio &&\r\n        right_to_left_face_ratio < this.left_right_turning_ratio\r\n      ) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"neutral\";\r\n      } else if (right_to_left_face_ratio > this.left_right_turning_ratio) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"right\";\r\n      } else if (right_to_left_face_ratio < 1 / this.left_right_turning_ratio) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"left\";\r\n      }\r\n      if (chin_slope_average > this.up_slope_threshold) {\r\n        this.liveness_detection_status[\"up_down_status\"] = \"up\";\r\n      } else if (chin_slope_average < this.down_slope_threshold) {\r\n        this.liveness_detection_status[\"up_down_status\"] = \"down\";\r\n      } else {\r\n        this.liveness_detection_status[\"up_down_status\"] = \"neutral\";\r\n      }\r\n    },\r\n    mouth_eye_status: function() {\r\n      //  https://towardsdatascience.com/mouse-control-facial-movements-hci-app-c16b0494a971\r\n      var landmarks = this.detections.landmarks;\r\n      const landmarkPositions = landmarks.positions;\r\n      // https://www.pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/\r\n      const mouth_y = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[51]),\r\n        Object.values(landmarkPositions[57])\r\n      );\r\n      const mouth_x = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[54]),\r\n        Object.values(landmarkPositions[48])\r\n      );\r\n      const mouth_aspect_ratio = mouth_y / mouth_x;\r\n      if (mouth_aspect_ratio > this.mouth_threshold) {\r\n        this.liveness_detection_status.mouth = \"open\";\r\n      } else {\r\n        this.liveness_detection_status.mouth = \"neutral\";\r\n      }\r\n\r\n      //  ------------- eyebrow measurment: use the distance between eyebrow and eye vs\r\n      // the width of the eyes (rather constant)  to test whether the eyebrows are raised\r\n      const l_eyebrow_eye_d = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[19]),\r\n        Object.values(landmarkPositions[37])\r\n      );\r\n      const r_eyebrow_eye_d = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[24]),\r\n        Object.values(landmarkPositions[44])\r\n      );\r\n      // use eyebrow as a reference point\r\n      const l_eye_w = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[39]),\r\n        Object.values(landmarkPositions[36])\r\n      );\r\n      const r_eye_w = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[42]),\r\n        Object.values(landmarkPositions[45])\r\n      );\r\n      const eyebrow_d_eye_w_ratio =\r\n        (l_eyebrow_eye_d + r_eyebrow_eye_d) / (l_eye_w + r_eye_w);\r\n\r\n      if (eyebrow_d_eye_w_ratio > this.eyebrow_threshold) {\r\n        this.liveness_detection_status.eyebrow = \"raised\";\r\n      } else {\r\n        this.liveness_detection_status.eyebrow = \"neutral\";\r\n      }\r\n    },\r\n    download_face_descriptor: function() {\r\n      console.log(this.detections);\r\n      var detections_json = JSON.stringify(this.detections);\r\n      var blob = new Blob([detections_json], { type: \"application/json\" });\r\n      FileSaver.saveAs(blob, \"key.json\");\r\n    },\r\n  }\r\n};\r\n",{"version":3,"sources":["FaceDetection.vue"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAw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file":"FaceDetection.vue","sourceRoot":"src/components","sourcesContent":["<template>\r\n  <div>\r\n    <v-container id=\"overlay\" fluid v-if=\"!detected\">\r\n      <v-layout align-center justify-center row fill-height id=\"overlay_content\">\r\n        <v-flex xs1>\r\n          <v-fa-icon name=\"robot\" scale=\"3\"/>\r\n        </v-flex>\r\n        <v-flex xs10>\r\n          <label class=\"display-3\">AI is getting ready, let the camera see you</label>\r\n        </v-flex>\r\n        <v-flex xs1>\r\n          <v-fa-icon name=\"camera\" scale=\"3\"/>\r\n        </v-flex>\r\n      </v-layout>\r\n    </v-container>\r\n    <v-container grid-list-md text-xs-center fluid>\r\n      <v-layout justify-center v-if=\"current_instruction.command.length != 0\" class=\"display-2\">\r\n        <label>\r\n          Instruction: {{current_instruction.command}}\r\n          <v-fa-icon :name=\"current_instruction.icon\" scale=\"3\"/>\r\n        </label>\r\n      </v-layout>\r\n      <v-layout justify-center>\r\n        <video\r\n          id=\"video\"\r\n          ref=\"video\"\r\n          :width=\"display_size.width\"\r\n          :height=\"display_size.height\"\r\n          v-if=\"true\"\r\n          autoplay\r\n          @play=\"detect_and_draw_faces\"\r\n        ></video>\r\n\r\n        <canvas\r\n          id=\"canvas_flip\"\r\n          ref=\"canvas_flip\"\r\n          :width=\"display_size.width\"\r\n          :height=\"display_size.height\"\r\n        />\r\n        <canvas id=\"canvas\" ref=\"canvas\" :width=\"display_size.width\" :height=\"display_size.height\"/>\r\n        <!-- third canvas just for image capturing -->\r\n        <canvas\r\n          id=\"canvas_capture\"\r\n          ref=\"canvas_capture\"\r\n          :width=\"display_size.width\"\r\n          :height=\"display_size.height\"\r\n        />\r\n      </v-layout>\r\n      <v-layout align-center justify-center>\r\n        <v-flex xs4></v-flex>\r\n      </v-layout>\r\n    </v-container>\r\n  </div>\r\n</template>\r\n\r\n<script>\r\n// useful tutorial: https://www.youtube.com/watch?v=CVClHLwv-4I&feature=youtu.be\r\n// https://github.com/WebDevSimplified/Face-Detection-JavaScript/blob/master/script.js\r\n\r\nimport * as faceapi from \"face-api.js\";\r\nimport { async } from \"q\";\r\nimport { setTimeout, setInterval } from \"timers\";\r\nimport FileSaver from \"file-saver\";\r\nimport Icon from \"vue-awesome/components/Icon\";\r\nimport vue2Dropzone from \"vue2-dropzone\";\r\nimport \"vue2-dropzone/dist/vue2Dropzone.min.css\";\r\n\r\nexport default {\r\n  name: \"FaceDetection\",\r\n  components: {\r\n    \"v-fa-icon\": Icon,\r\n    vueDropzone: vue2Dropzone\r\n  },\r\n  data: function() {\r\n    return {\r\n      display_size: { width: 640, height: 480 },\r\n      detections: [],\r\n      temp_detections: [],\r\n      capture_file_count: 0,\r\n      detected: false,\r\n      resizedDetections: [],\r\n      confirmed_user: \"\",\r\n      refresh_time: 50,\r\n      // determine sensitivity for left and right face, higher = more turning needed\r\n      left_right_turning_ratio: 2,\r\n      // determine sensitivity for look up, higher = more turning needed\r\n      up_slope_threshold: -0.1,\r\n      down_slope_threshold: -0.55,\r\n      // larger the more opening\r\n      mouth_threshold: 0.7,\r\n      // larger the more opening\r\n      eyebrow_threshold: 1.0,\r\n      // List of available instructions\r\n      instructions: [\r\n        { command: \"Turn Left!\", icon: \"arrow-alt-circle-left\" },\r\n        { command: \"Turn Right!\", icon: \"arrow-alt-circle-right\" },\r\n        { command: \"Look Up!\", icon: \"arrow-alt-circle-up\" },\r\n        { command: \"Look Down!\", icon: \"arrow-alt-circle-down\" },\r\n        { command: \"Smile!\", icon: \"laugh\" },\r\n        { command: \"Open mouth!\", icon: \"teeth-open\" }\r\n\r\n        // \"Raise eyebrows!\"\r\n      ],\r\n      current_instruction: { command: \"\", icon: \"\" },\r\n      liveness_detection_status: {\r\n        left_right_status: \"neutral\",\r\n        up_down_status: \"neutral\",\r\n        expression: \"neutral\",\r\n        mouth: \"neutral\",\r\n        eyebrow: \"neutral\"\r\n      },\r\n      liveness_test_instructions: []\r\n    };\r\n  },\r\n  watch: {\r\n    temp_detections: function(new_val, old_val) {\r\n      console.log(new_val);\r\n    }\r\n  },\r\n  mounted: function() {\r\n    this.load_faceapi_models();\r\n    this.start_video();\r\n    this.generate_liveness_test();\r\n  },\r\n  computed: {\r\n    session_id() {\r\n      // session_id should be sent to all the axios request to backend now\r\n      return this.$store.state.session_id;\r\n    },\r\n      user_name_in_key(){\r\n      return this.$store.state.user_name_in_key;\r\n    }\r\n  },\r\n  methods: {\r\n    generate_liveness_test: function() {\r\n      // DEBUG: add a timer for each test?\r\n      function shuffleArray(array) {\r\n        for (let i = array.length - 1; i > 0; i--) {\r\n          const j = Math.floor(Math.random() * (i + 1));\r\n          [array[i], array[j]] = [array[j], array[i]];\r\n        }\r\n      }\r\n      if (this.instructions.length == 0) {\r\n        console.log(\"you are a human!\");\r\n        this.current_instruction.command = \"\";\r\n        this.match_known_descriptor();\r\n      } else {\r\n        shuffleArray(this.instructions);\r\n        this.current_instruction = this.instructions[0];\r\n      }\r\n    },\r\n    liveness_test: function() {\r\n      var test = this.current_instruction.command;\r\n      var result = this.liveness_detection_status;\r\n      // use the neutral pose to ensure we don't get false detection\r\n      var neutral_l_r_pose = result.left_right_status == \"neutral\";\r\n      var neutral_u_d_pose = result.up_down_status == \"neutral\";\r\n      var neutral_mouth = result.mouth == \"neutral\";\r\n\r\n      if (test == \"Turn Left!\" && result.left_right_status == \"left\") {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      } else if (test == \"Turn Right!\" && result.left_right_status == \"right\") {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      } else if (\r\n        test == \"Look Up!\" &&\r\n        result.up_down_status == \"up\" &&\r\n        neutral_l_r_pose &&\r\n        neutral_mouth\r\n      ) {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      } else if (\r\n        test == \"Look Down!\" &&\r\n        result.up_down_status == \"down\" &&\r\n        neutral_l_r_pose &&\r\n        neutral_mouth\r\n      ) {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      } else if (\r\n        test == \"Smile!\" &&\r\n        result.expression == \"happy\" &&\r\n        neutral_l_r_pose &&\r\n        neutral_u_d_pose\r\n      ) {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      } else if (\r\n        test == \"Open mouth!\" &&\r\n        result.mouth == \"open\" &&\r\n        neutral_l_r_pose\r\n      ) {\r\n        this.generate_face_descriptor();\r\n        return true;\r\n      }\r\n      // } else if (\r\n      //   test == \"Raise eyebrows!\" &&\r\n      //   result.eyebrow == \"raised\" &&\r\n      //   neutral_l_r_pose &&\r\n      //   neutral_u_d_pose\r\n      // ) {\r\n      //   return true;\r\n      // }\r\n    },\r\n\r\n    load_faceapi_models: function() {\r\n      // save the models to the public/models\r\n      // faceapi.nets.ssdMobilenetv1.loadFromUri(\"/models\"),\r\n      Promise.all([\r\n        faceapi.nets.tinyFaceDetector.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceLandmark68Net.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceRecognitionNet.loadFromUri(\"/models\"),\r\n        faceapi.nets.faceExpressionNet.loadFromUri(\"/models\")\r\n      ]).then(console.log(\"model loaded\"));\r\n    },\r\n    start_video: function() {\r\n      navigator.getUserMedia(\r\n        { video: {} },\r\n        stream => (video.srcObject = stream),\r\n        err => console.error(err)\r\n      );\r\n    },\r\n    generate_face_descriptor: function() {\r\n      // https://x-team.com/blog/webcam-capture-vue/\r\n      let self = this;\r\n      if (self.detections) {\r\n        // keep track how many images we have captured\r\n        self.capture_file_count += 1;\r\n        // console.log(this.detections);\r\n        console.log(self.capture_file_count);\r\n        const canvas = self.$refs.canvas_capture;\r\n        var context = canvas.getContext(\"2d\");\r\n        context.clearRect(0, 0, canvas.width, canvas.height);\r\n        var box = self.detections.detection.box;\r\n        // giving some padding so we dont crop too much which gives problem to backend\r\n        console.log(box)\r\n        console.log([canvas.width, canvas.height])\r\n\r\n\r\n        // Sending the face with cropped region \r\n        // drawImage from the video stream, with cropping\r\n        context.drawImage(\r\n          self.$refs.video,\r\n          box.x,\r\n          box.y,\r\n          box.width,\r\n          box.height,\r\n          box.x,\r\n          box.y,\r\n          box.width,\r\n          box.height\r\n        );\r\n\r\n        // // sending the whole image \r\n        //   context.drawImage(\r\n        //   self.$refs.video,\r\n        //   0, 0, self.display_size['width'], self.display_size['height']\r\n        // );\r\n\r\n        // // // sending the face_location to backend to save some repetition \r\n        // // // top, right, bottom, left\r\n        const face_location = [box.top, box.right, box.bottom, box.left]\r\n\r\n\r\n\r\n\r\n        // Saving canvas to local drive is easy: https://github.com/eligrey/FileSaver.js/\r\n        self.$refs.canvas_capture.toBlob(async function(blob) {\r\n          // result = await faceapi.bufferToImage(blob).\r\n          // upload Blob as a form to the flask backend and generate descriptors\r\n          // https://github.com/pagekit/vue-resource/blob/master/docs/recipes.md\r\n          let formData = new FormData();\r\n          // formData.append(name, value, filename);\r\n          formData.append(\r\n            \"file\",\r\n            blob,\r\n            \"unknown_face_\" + self.capture_file_count + \".jpg\"\r\n          );\r\n          formData.append(\"session_id\", self.session_id);\r\n          formData.append(\"face_location\", face_location);\r\n          self.axios({\r\n            url: self.$API_URL + \"/generate_descriptor\",\r\n            method: \"POST\",\r\n            data: formData\r\n          });\r\n        });\r\n      }\r\n    },\r\n    match_known_descriptor: function() {\r\n      this.axios({\r\n        url: this.$API_URL + \"/compare_descriptors\",\r\n        method: \"POST\",\r\n        data: {session_id: this.session_id}\r\n      }).then(response => {\r\n        if (response.data.match) {\r\n          this.confirmed_user = response.data.user;\r\n        }\r\n      });\r\n    },\r\n    show_capture: function() {\r\n      console.log(this.temp_detections);\r\n    },\r\n    detect_and_draw_faces: function() {\r\n      console.log(\"start detecting faces\");\r\n      // https://michaelnthiessen.com/this-is-undefined/\r\n      // https://stackoverflow.com/questions/47148363/when-to-use-vm-or-this-in-vue-js\r\n      // arrow function is a pain in vue.js and this async function also seems to cause problem\r\n      let self = this;\r\n\r\n      setInterval(async function() {\r\n        //  After face detection and facial landmark prediction the face descriptors\r\n        //  all the results are stored in self.detections\r\n        self.detections = await faceapi\r\n          .detectSingleFace(\r\n            self.$refs.video,\r\n            new faceapi.TinyFaceDetectorOptions()\r\n          )\r\n          .withFaceLandmarks()\r\n          .withFaceExpressions();\r\n        // calculating the 128 descriptors seems to be rather slow, lets not do it very frequent or do it in backend\r\n        // .withFaceDescriptors();\r\n\r\n        if (self.detections) {\r\n          self.detected = true;\r\n          // calculated the size that is on our canvas size\r\n          const resizedDetections = faceapi.resizeResults(\r\n            self.detections,\r\n            self.display_size\r\n          );\r\n\r\n          // replace the result with scaled one, all the other information will remain\r\n          self.detections = resizedDetections;\r\n\r\n          // call other methods\r\n          self.mouth_eye_status();\r\n          self.head_pose_estimation();\r\n          self.find_expression();\r\n\r\n          // annotation\r\n          var message2 = [\r\n            \"L/R: \" + self.liveness_detection_status.left_right_status,\r\n            \"U/D: \" + self.liveness_detection_status.up_down_status,\r\n            \"Expression: \" + self.liveness_detection_status.expression,\r\n            \"mouth: \" + self.liveness_detection_status.mouth,\r\n            \"eyebrow: \" + self.liveness_detection_status.eyebrow\r\n          ];\r\n          var message = \"Hello\";\r\n          // only append the username after liveness test\r\n          if (self.instructions.length == 0 && self.confirmed_user != \"\") {\r\n            message = \"Hello: \" + self.user_name_in_key + \"!\";\r\n          } else if (\r\n            self.instructions.length == 0 &&\r\n            self.confirmed_user == \"\"\r\n          ) {\r\n            message = \"Hello: Stranger! Maybe you are not who you claim to be\";\r\n          }\r\n\r\n          self.annotation({\r\n            clear: false,\r\n            message: message,\r\n            message2: message2\r\n          });\r\n\r\n          // liveness test is passed, we move on with next test\r\n          if (self.instructions.length != 0) {\r\n            if (self.liveness_test()) {\r\n              self.instructions.shift();\r\n              self.generate_liveness_test();\r\n              console.log(\"Passed one test, now move on!\");\r\n            }\r\n          }\r\n        } else {\r\n          self.annotation({ clear: true });\r\n        }\r\n      }, self.refresh_time);\r\n    },\r\n    // note how to use named arguments in javascript...\r\n    annotation: function({ clear = false, message = \"Hello!\", message2 = [] }) {\r\n      var canvas = this.$refs.canvas;\r\n      var canvas_flip = this.$refs.canvas_flip;\r\n\r\n      // clear canvas before drawing the new things to prevent cluttering\r\n      canvas.getContext(\"2d\").clearRect(0, 0, canvas.width, canvas.height);\r\n      canvas_flip\r\n        .getContext(\"2d\")\r\n        .clearRect(0, 0, canvas_flip.width, canvas_flip.height);\r\n\r\n      // set to clear when no face can be found\r\n      if (clear === false) {\r\n        // NOTE: There are some useful drawing functions already exist\r\n        faceapi.draw.drawFaceLandmarks(canvas_flip, this.detections);\r\n\r\n        // Draw box around the face with 1 line text\r\n        const box = this.detections.detection.box;\r\n        // mirror X\r\n        const box_X_flipped = {\r\n          x: canvas.width - box.x - box.width,\r\n          y: box.y,\r\n          width: box.width,\r\n          height: box.height\r\n        };\r\n        // see DrawBoxOptions below\r\n        const boxOptions = {\r\n          label: message,\r\n          lineWidth: 2,\r\n          boxColor: \"rgba(57,255,20,0.8)\"\r\n        };\r\n        // flip the x-value x, y, width, height\r\n        const drawBox = new faceapi.draw.DrawBox(box_X_flipped, boxOptions);\r\n        drawBox.draw(canvas);\r\n\r\n        // ------- multi-line text at bottom -------\r\n        if (message2.length != 0) {\r\n          const text = message2;\r\n          const anchor = this.detections.detection.box.bottomLeft;\r\n          const anchor_X_flipped = {\r\n            x: canvas.width - anchor.x - box.width,\r\n            y: anchor.y\r\n          };\r\n          // see DrawTextField below\r\n          const textOptions = {\r\n            anchorPosition: \"TOP_LEFT\",\r\n            backgroundColor: \"rgba(57,255,20, 0.5)\"\r\n          };\r\n          const drawText = new faceapi.draw.DrawTextField(\r\n            text,\r\n            anchor_X_flipped,\r\n            textOptions\r\n          );\r\n          drawText.draw(canvas);\r\n        }\r\n      }\r\n    },\r\n    find_expression: function() {\r\n      var expressions = this.detections.expressions;\r\n      // find the max value of all the keys\r\n      // The reducer function takes four arguments: Accumulator (acc), Current Value (cur), Current Index (idx)\r\n      // the question mark is conditional operator, which returns true_val:false_val\r\n      var likely_expression = Object.keys(expressions).reduce((i, j) =>\r\n        expressions[i] > expressions[j] ? i : j\r\n      );\r\n      var expression_possibility = expressions[likely_expression];\r\n      this.liveness_detection_status[\"expression\"] = likely_expression;\r\n\r\n      return likely_expression + \":\" + String(expression_possibility);\r\n    },\r\n    head_pose_estimation: function() {\r\n      const landmarks = this.detections.landmarks;\r\n      const landmarkPositions = landmarks.positions;\r\n\r\n      // the distance between 1 and 28 will be shorter when extending left face\r\n      const right_face_distance = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[16]),\r\n        Object.values(landmarkPositions[28])\r\n      );\r\n      const left_face_distance = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[0]),\r\n        Object.values(landmarkPositions[28])\r\n      );\r\n      const right_to_left_face_ratio = right_face_distance / left_face_distance;\r\n\r\n      // the point 6 - 12 become more flat when facing upward. calculate the slope\r\n      const chin_slope_left =\r\n        (landmarkPositions[5].y - landmarkPositions[8].y) /\r\n        (landmarkPositions[8].x - landmarkPositions[5].x);\r\n      const chin_slope_right =\r\n        (landmarkPositions[11].y - landmarkPositions[8].y) /\r\n        (landmarkPositions[11].x - landmarkPositions[8].x);\r\n      const chin_slope_average = (chin_slope_left + chin_slope_right) / 2;\r\n      // NOTE: make this vue variable\r\n\r\n      if (\r\n        right_to_left_face_ratio > 1 / this.left_right_turning_ratio &&\r\n        right_to_left_face_ratio < this.left_right_turning_ratio\r\n      ) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"neutral\";\r\n      } else if (right_to_left_face_ratio > this.left_right_turning_ratio) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"right\";\r\n      } else if (right_to_left_face_ratio < 1 / this.left_right_turning_ratio) {\r\n        this.liveness_detection_status[\"left_right_status\"] = \"left\";\r\n      }\r\n      if (chin_slope_average > this.up_slope_threshold) {\r\n        this.liveness_detection_status[\"up_down_status\"] = \"up\";\r\n      } else if (chin_slope_average < this.down_slope_threshold) {\r\n        this.liveness_detection_status[\"up_down_status\"] = \"down\";\r\n      } else {\r\n        this.liveness_detection_status[\"up_down_status\"] = \"neutral\";\r\n      }\r\n    },\r\n    mouth_eye_status: function() {\r\n      //  https://towardsdatascience.com/mouse-control-facial-movements-hci-app-c16b0494a971\r\n      var landmarks = this.detections.landmarks;\r\n      const landmarkPositions = landmarks.positions;\r\n      // https://www.pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/\r\n      const mouth_y = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[51]),\r\n        Object.values(landmarkPositions[57])\r\n      );\r\n      const mouth_x = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[54]),\r\n        Object.values(landmarkPositions[48])\r\n      );\r\n      const mouth_aspect_ratio = mouth_y / mouth_x;\r\n      if (mouth_aspect_ratio > this.mouth_threshold) {\r\n        this.liveness_detection_status.mouth = \"open\";\r\n      } else {\r\n        this.liveness_detection_status.mouth = \"neutral\";\r\n      }\r\n\r\n      //  ------------- eyebrow measurment: use the distance between eyebrow and eye vs\r\n      // the width of the eyes (rather constant)  to test whether the eyebrows are raised\r\n      const l_eyebrow_eye_d = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[19]),\r\n        Object.values(landmarkPositions[37])\r\n      );\r\n      const r_eyebrow_eye_d = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[24]),\r\n        Object.values(landmarkPositions[44])\r\n      );\r\n      // use eyebrow as a reference point\r\n      const l_eye_w = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[39]),\r\n        Object.values(landmarkPositions[36])\r\n      );\r\n      const r_eye_w = faceapi.euclideanDistance(\r\n        Object.values(landmarkPositions[42]),\r\n        Object.values(landmarkPositions[45])\r\n      );\r\n      const eyebrow_d_eye_w_ratio =\r\n        (l_eyebrow_eye_d + r_eyebrow_eye_d) / (l_eye_w + r_eye_w);\r\n\r\n      if (eyebrow_d_eye_w_ratio > this.eyebrow_threshold) {\r\n        this.liveness_detection_status.eyebrow = \"raised\";\r\n      } else {\r\n        this.liveness_detection_status.eyebrow = \"neutral\";\r\n      }\r\n    },\r\n    download_face_descriptor: function() {\r\n      console.log(this.detections);\r\n      var detections_json = JSON.stringify(this.detections);\r\n      var blob = new Blob([detections_json], { type: \"application/json\" });\r\n      FileSaver.saveAs(blob, \"key.json\");\r\n    },\r\n  }\r\n};\r\n</script>\r\n\r\n<style scoped>\r\n@import url(\"https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css\");\r\n\r\n#video {\r\n  position: absolute;\r\n  /* flip the image so it is more natural */\r\n  transform: scaleX(-1);\r\n}\r\n\r\n#canvas {\r\n  /* for some reason, we dont need both to be absolute? */\r\n  /* position: absolute; */\r\n  pointer-events: none;\r\n  z-index: 2;\r\n}\r\n\r\n#canvas_flip {\r\n  position: absolute;\r\n  pointer-events: none;\r\n  z-index: 2;\r\n  transform: scaleX(-1);\r\n}\r\n#canvas_capture {\r\n  /* we dont need to see it */\r\n  position: absolute;\r\n  display: none;\r\n  z-index: 3;\r\n  transform: scaleX(-1);\r\n}\r\n\r\n#overlay {\r\n  position: fixed;\r\n  width: 100%; /* Full width (cover the whole page) */\r\n  height: 100%; /* Full height (cover the whole page) */\r\n  top: 0;\r\n  left: 0;\r\n  right: 0;\r\n  bottom: 0;\r\n  background: rgb(0, 0, 0) center center no-repeat;\r\n  opacity: 1;\r\n  z-index: 5;\r\n}\r\n\r\n#overlay_content {\r\n  z-index: 6;\r\n  opacity: 0.8;\r\n  color: white;\r\n}\r\n</style>\r\n"]}]}